pipeline for the code to run
1: get your dataset done in dataset/...
(gsm)
python template_choice.py --data_path dataset/multiple_choice/gsm_test.json --save_path dataset/multiple_choice/processed_gsm_test.json

python template.py --data_path dataset/blank/gsm_test.json --save_path dataset/blank/processed_gsm_test.json

(MATH)

(pararel)
python template_choice.py --data_path dataset/multiple_choice/pararel_test.json --save_path dataset/multiple_choice/processed_pararel_test.json --length 30000

python template.py --data_path dataset/blank/pararel_test.json --save_path dataset/blank/processed_pararel_test.json

(MMLU-PRO)
python template_choice.py --data_path dataset/multiple_choice/mmlu_pro_test.json --save_path dataset/multiple_choice/processed_mmlu_pro_test.json

(MMLU)
python template_choice.py --data_path dataset/multiple_choice/mmlu_test.json --save_path dataset/multiple_choice/processed_mmlu_test.json

2: generate the output
(gsm)
python generate_output.py --data_path dataset/multiple_choice/processed_gsm_test.json --save_path result/multiple_choice/gsm.json --generate_vllm

python generate_output.py --data_path dataset/blank/processed_gsm_test.json --save_path result/blank/gsm.json --case blank --batch_size 4 --generate_vllm --gpu 0

(pararel)
python generate_output.py --data_path dataset/multiple_choice/processed_pararel_test.json --save_path result/multiple_choice/pararel.json --generate_vllm --gpu 3

python generate_output.py --data_path dataset/blank/processed_pararel_test.json --save_path result/blank/pararel.json --case blank --generate_vllm --gpu 2

(MMLU-PRO)
python generate_output.py --data_path dataset/multiple_choice/processed_mmlu_pro_test.json --save_path result/multiple_choice/mmlu_pro.json  --gpu 2

(MMLU)
python generate_output.py --data_path dataset/multiple_choice/processed_mmlu_test.json --save_path result/multiple_choice/mmlu.json  --gpu 2

3: compare the output and ground truth label
(multiple_choice)
python multiple_choice_compare.py --data_path result/multiple_choice/gsm.json

python multiple_choice_compare.py --data_path result/multiple_choice/pararel.json

python multiple_choice_compare.py --data_path result/multiple_choice/mmlu_pro.json

python multiple_choice_compare.py --data_path result/multiple_choice/mmlu.json

(LLM-based)
python LLM_judge.py --data_path result/blank/gsm.json --gpu 0

python LLM_judge.py --data_path result/blank/pararel.json --gpu 0

python LLM_judge.py --data_path result/multiple_choice/mmlu.json --save_path result/test.json --batch_size 16 --gpu 0
------------------------------------------------------------------------------------------------------------
4: divide the dataset into certain and uncertain
(gsm)
python divide_dataset.py --data_path dataset/multiple_choice/processed_gsm_test.json --result result/multiple_choice/gsm.json --save_path dataset/multiple_choice/gsm_split/gsm --case choice

python divide_dataset.py --data_path dataset/blank/processed_gsm_test.json --result result/blank/gsm.json --save_path dataset/blank/gsm_split/gsm --case blank

(pararel)
python divide_dataset.py --data_path dataset/blank/processed_pararel_test.json --result result/blank/pararel.json --save_path dataset/blank/pararel_split/pararel --case blank


5: fine-tune the model and save and model
(gsm)
python fine_tune.py --data_path dataset/blank/gsm_split/gsm --save_path models/llama3_gsm --batch_size 1 --case blank --gpu 0


6: generate the result again
(gsm)
python generate_output.py --data_path dataset/blank/processed_gsm_test.json --save_path fine_tune_result/test.json --case blank --batch_size 4 --lora_model --gpu 0


7: compare the output with the ground truth label again
